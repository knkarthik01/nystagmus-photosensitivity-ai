{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knkarthik01/nystagmus-photosensitivity-ai/blob/main/notebooks/2_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114850fb",
      "metadata": {
        "id": "114850fb"
      },
      "source": [
        "# üìì 2_Model_Training.ipynb\n",
        "\n",
        "Train a Dual-Branch CNN model on synthetic photosensitivity risk dataset.\n",
        "\n",
        "üöÄ Working with Data (CSV Handling) - Flexible Approaches\n",
        "After running 1_data_preprocessing.ipynb, you have two options to make preprocessed_data.csv available for the next steps:\n",
        "\n",
        "## Option 1: Manual Upload (Recommended)\n",
        "Download preprocessed_data.csv from Colab:\n",
        "\n",
        "Click on the folder üìÅ icon (sidebar)\n",
        "\n",
        "Right-click preprocessed_data.csv ‚Üí Download\n",
        "\n",
        "Upload to GitHub:\n",
        "\n",
        "Go to your repo ‚Üí Navigate to data/preprocessed/\n",
        "\n",
        "Click Add file ‚Üí Upload files\n",
        "\n",
        "Commit the uploaded file.\n",
        "\n",
        "In 2_model_training.ipynb, load it like:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "data = pd.read_csv('data/preprocessed/preprocessed_data.csv')\n",
        "\n",
        "### Option 2: Temporary Usage (within same Colab session)\n",
        "If you plan to run all notebooks one after another in the same Colab session, the file stays available.\n",
        "\n",
        "Simply run 1_data_preprocessing.ipynb first, then 2_model_training.ipynb.\n",
        "\n",
        "### Load it directly:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "data = pd.read_csv('preprocessed_data.csv')\n",
        "\n",
        "‚ö° Note: If you disconnect the Colab runtime, the file is lost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8d222aa1",
      "metadata": {
        "id": "8d222aa1"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65597805",
      "metadata": {
        "id": "65597805"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load preprocessed data\n",
        "data = pd.read_csv('preprocessed_data.csv')\n",
        "X = data[['brightness_level', 'eye_movement_variance']].values\n",
        "y = data['photosensitivity_risk'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-ees6CJzyFb",
        "outputId": "feb8df21-7cb6-4ccc-a8b1-20f6d41f89c3"
      },
      "id": "r-ees6CJzyFb",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b91fdb90",
      "metadata": {
        "id": "b91fdb90"
      },
      "outputs": [],
      "source": [
        "# Step 3: Prepare PyTorch datasets\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ad7a104e",
      "metadata": {
        "id": "ad7a104e"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define the Dual-Branch CNN model (simple version)\n",
        "class DualBranchNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualBranchNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "model = DualBranchNet()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ab25c3fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab25c3fb",
        "outputId": "344619d1-d38d-4cdb-8471-8ee4153409bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.1980\n",
            "Epoch 2, Loss: 1.1958\n",
            "Epoch 3, Loss: 0.8467\n",
            "Epoch 4, Loss: 0.7264\n",
            "Epoch 5, Loss: 0.7085\n",
            "Epoch 6, Loss: 0.7176\n",
            "Epoch 7, Loss: 0.7111\n",
            "Epoch 8, Loss: 0.7172\n",
            "Epoch 9, Loss: 0.7055\n",
            "Epoch 10, Loss: 0.7086\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d9ea67a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d9ea67a",
        "outputId": "a5fc20f6-c6dc-4af2-e67a-b72cb41f9819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model trained and saved as dual_branch_cnn.pth\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Save the model\n",
        "torch.save(model.state_dict(), 'dual_branch_cnn.pth')\n",
        "print('‚úÖ Model trained and saved as dual_branch_cnn.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}